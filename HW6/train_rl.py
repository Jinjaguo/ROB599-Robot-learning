import matplotlib.pyplot as plt
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback
from panda_pushing_env import PandaPushingEnv, TARGET_POSE, OBSTACLE_CENTRE, OBSTACLE_HALFDIMS, BOX_SIZE

from pushing_rl import *

# ğŸ’¾ ç”¨äºä¿å­˜ reward çš„ callback
class RewardTrackingCallback(BaseCallback):
    def __init__(self, verbose=0):
        super().__init__(verbose)
        self.rewards = []

    def _on_step(self) -> bool:
        if len(self.locals["infos"]) > 0 and "episode" in self.locals["infos"][0]:
            ep_rew = self.locals["infos"][0]["episode"]["r"]
            self.rewards.append(ep_rew)
        return True

# âœ… é…ç½®è®­ç»ƒå‚æ•°
total_timesteps = 25000
env = PandaPushingEnv(
    state_space='object_pose',
    reward_function=obstacle_free_pushing_reward_function_object_pose_space
)
env.reset()

# âœ… å®ä¾‹åŒ– PPOï¼ˆå¼ºåˆ¶ä½¿ç”¨ CPUï¼Œæ—¥å¿—ç›®å½•å¯ç”¨ TensorBoardï¼‰
model = PPO("MlpPolicy", env, verbose=1, device="cpu", tensorboard_log="./logs", learning_rate=1e-4)

# âœ… å¯åŠ¨è®­ç»ƒ
reward_callback = RewardTrackingCallback()
model.learn(total_timesteps=total_timesteps, progress_bar=True, callback=reward_callback)

# âœ… ä¿å­˜æ¨¡å‹
model.save("free_pushing_object_pose")

# ğŸ“Š ç”» reward æ›²çº¿ï¼ˆå¦‚æœæœ‰è®°å½•ï¼‰
if reward_callback.rewards:
    plt.plot(reward_callback.rewards)
    plt.xlabel("Episode")
    plt.ylabel("Reward")
    plt.title("Reward Curve During Training")
    plt.grid(True)
    plt.tight_layout()
    plt.savefig("reward_curve.png")  # ä¿å­˜å›¾åƒ
    plt.show()
else:
    print("âš ï¸ æ²¡æœ‰è®°å½•åˆ° rewardï¼Œå¯èƒ½æ˜¯ environment çš„ info æ²¡åŒ…å« 'episode' ä¿¡æ¯")
